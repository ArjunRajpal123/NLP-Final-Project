{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting mindspore\n",
            "  Downloading mindspore-1.10.0-cp38-cp38-manylinux1_x86_64.whl (158.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 158.8 MB 53 bytes/s a 0:00:011\n",
            "\u001b[?25hRequirement already satisfied: scipy>=1.5.2 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from mindspore) (1.5.3)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from mindspore) (9.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from mindspore) (21.3)\n",
            "Requirement already satisfied: protobuf>=3.13.0 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from mindspore) (3.20.1)\n",
            "Requirement already satisfied: astunparse>=1.6.3 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from mindspore) (1.6.3)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from mindspore) (1.21.6)\n",
            "Requirement already satisfied: psutil>=5.6.1 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from mindspore) (5.9.4)\n",
            "Requirement already satisfied: asttokens>=2.0.4 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from mindspore) (2.2.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from packaging>=20.0->mindspore) (3.0.9)\n",
            "Requirement already satisfied: six<2.0,>=1.6.1 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from astunparse>=1.6.3->mindspore) (1.16.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from astunparse>=1.6.3->mindspore) (0.37.1)\n",
            "Installing collected packages: mindspore\n",
            "Successfully installed mindspore-1.10.0\n",
            "Collecting lda\n",
            "  Downloading lda-2.0.0-cp38-cp38-manylinux1_x86_64.whl (348 kB)\n",
            "\u001b[K     |████████████████████████████████| 348 kB 6.0 MB/s eta 0:00:01\n",
            "\u001b[?25hCollecting pbr<4,>=0.6\n",
            "  Downloading pbr-3.1.1-py2.py3-none-any.whl (99 kB)\n",
            "\u001b[K     |████████████████████████████████| 99 kB 1.7 MB/s  eta 0:00:01\n",
            "\u001b[?25hRequirement already satisfied: numpy<2.0,>=1.13.0 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from lda) (1.21.6)\n",
            "Installing collected packages: pbr, lda\n",
            "Successfully installed lda-2.0.0 pbr-3.1.1\n",
            "Requirement already satisfied: pandas in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (1.1.5)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from pandas) (1.21.6)\n",
            "Requirement already satisfied: pytz>=2017.2 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from pandas) (2022.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas) (1.16.0)\n",
            "Requirement already satisfied: numpy in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (1.21.6)\n",
            "\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.VerifiedHTTPSConnection object at 0x7f46cf642160>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/nltk/\u001b[0m\n",
            "Collecting nltk\n",
            "  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5 MB 5.5 MB/s eta 0:00:01\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from nltk) (4.64.1)\n",
            "Requirement already satisfied: click in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from nltk) (8.1.3)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from nltk) (2022.10.31)\n",
            "Requirement already satisfied: joblib in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from nltk) (0.14.1)\n",
            "Installing collected packages: nltk\n",
            "Successfully installed nltk-3.8.1\n",
            "Requirement already satisfied: scikit-learn in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (0.22.1)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from scikit-learn) (1.21.6)\n",
            "Requirement already satisfied: joblib>=0.11 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from scikit-learn) (0.14.1)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from scikit-learn) (1.5.3)\n",
            "Requirement already satisfied: scipy in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (1.5.3)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from scipy) (1.21.6)\n",
            "Requirement already satisfied: torch in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (1.12.0)\n",
            "Requirement already satisfied: typing-extensions in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from torch) (4.4.0)\n",
            "Requirement already satisfied: gensim in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (3.8.3)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from gensim) (1.5.3)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from gensim) (1.9.0)\n",
            "Requirement already satisfied: six>=1.5.0 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from gensim) (1.16.0)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from gensim) (1.21.6)\n",
            "Requirement already satisfied: requests in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from smart-open>=1.8.1->gensim) (2.28.2)\n",
            "Requirement already satisfied: boto3 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from smart-open>=1.8.1->gensim) (1.20.19)\n",
            "Requirement already satisfied: boto>=2.32 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from smart-open>=1.8.1->gensim) (2.49.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from requests->smart-open>=1.8.1->gensim) (3.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from requests->smart-open>=1.8.1->gensim) (3.0.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from requests->smart-open>=1.8.1->gensim) (2022.9.24)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from requests->smart-open>=1.8.1->gensim) (1.26.14)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from boto3->smart-open>=1.8.1->gensim) (0.10.0)\n",
            "Requirement already satisfied: s3transfer<0.6.0,>=0.5.0 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from boto3->smart-open>=1.8.1->gensim) (0.5.2)\n",
            "Requirement already satisfied: botocore<1.24.0,>=1.23.19 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from boto3->smart-open>=1.8.1->gensim) (1.23.19)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from botocore<1.24.0,>=1.23.19->boto3->smart-open>=1.8.1->gensim) (2.8.2)\n",
            "Requirement already satisfied: regex in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (2022.10.31)\n"
          ]
        }
      ],
      "source": [
        "!pip install mindspore\n",
        "!pip install lda\n",
        "!pip install pandas\n",
        "!pip install numpy\n",
        "!pip install nltk\n",
        "!pip install scikit-learn\n",
        "!pip install scipy\n",
        "!pip install torch\n",
        "!pip install gensim\n",
        "!pip install regex"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "gather": {
          "logged": 1683406694683
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from gensim.test.utils import common_texts\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "gather": {
          "logged": 1683406809432
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /home/azureuser/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /home/azureuser/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import string\n",
        "import nltk\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "wn = WordNetLemmatizer()\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "gather": {
          "logged": 1683406120821
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Load the data\n",
        "train_data = pd.read_csv(\"arp_train.csv\")\n",
        "test_data = pd.read_csv(\"arp_test.csv\")\n",
        "\n",
        "# Remove any rows with missing values\n",
        "train_data.dropna(inplace=True)\n",
        "test_data.dropna(inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "gather": {
          "logged": 1683407063715
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "def clean_text(lines, review_lines):\n",
        "    for line in lines:\n",
        "        tokens = word_tokenize(line)\n",
        "        #convert to lower case\n",
        "        tokens = [w.lower() for w in tokens]\n",
        "        #remove punctuation from each word\n",
        "        table = str.maketrans('', '', string.punctuation)\n",
        "        stripped = [w.translate(table) for w in tokens]\n",
        "        #remove remaining tokens that are not alphabetic\n",
        "        words = [word for word in stripped if word.isalpha()]\n",
        "        #filter out stop words\n",
        "        stop_words = set(stopwords.words('english'))\n",
        "        words = [w for w in words if not w in stop_words]\n",
        "        words = [wn.lemmatize(w) for w in words]\n",
        "        review_lines.append(words)\n",
        "    return review_lines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1683406243887
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "clean_test = list()\n",
        "clean_text(test_data[\"review\"], clean_test)\n",
        "clean_train = list()\n",
        "clean_text(train_data[\"review\"], clean_train)\n",
        "\n",
        "train_docs = [' '.join(sublist) for sublist in clean_train]\n",
        "test_docs = [' '.join(sublist) for sublist in clean_test]\n",
        "print(\"done with cleaning data\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "gather": {
          "logged": 1683419658906
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "train_labels = train_data['label'].apply(lambda x: 1 if x == 'pos' else 0)\n",
        "\n",
        "test_labels = test_data['label'].apply(lambda x: 1 if x == 'pos' else 0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1683406243913
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "\n",
        "vectorizer = CountVectorizer()\n",
        "train_bow = vectorizer.fit_transform(train_docs)\n",
        "test_bow = vectorizer.transform(test_docs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1683406243924
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "print(\"hey\")\n",
        "lda = LatentDirichletAllocation(n_components=10, random_state=42)\n",
        "lda.fit(train_bow)\n",
        "print(1)\n",
        "#Generate sparse vectors (word embeddings) for the train, validation and test sets\n",
        "train_embeddings = lda.transform(train_bow)\n",
        "print(2)\n",
        "test_embeddings = lda.transform(test_bow)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1683406243954
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "lr = LogisticRegression(random_state=42)\n",
        "lr.fit(train_embeddings, train_labels)\n",
        "\n",
        "# Evaluate the model on the validation set\n",
        "val_preds = lr.predict(test_embeddings)\n",
        "val_acc = accuracy_score(test_labels, val_preds)\n",
        "val_f1 = f1_score(test_labels, val_preds, average='weighted')\n",
        "val_precision = precision_score(test_labels, val_preds, average='weighted')\n",
        "val_recall = recall_score(test_labels, val_preds, average='weighted')\n",
        "\n",
        "print(\"Validation set results:\")\n",
        "print(\"Accuracy: {:.4f}\".format(val_acc))\n",
        "print(\"F1 Score: {:.4f}\".format(val_f1))\n",
        "print(\"Precision: {:.4f}\".format(val_precision))\n",
        "print(\"Recall: {:.4f}\".format(val_recall))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1683406243937
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from sklearn.svm import SVC\n",
        "# Initialize an SVM classifier\n",
        "svm = SVC(random_state=42)\n",
        "\n",
        "# Fit the classifier on the training set\n",
        "svm.fit(train_embeddings, train_labels)\n",
        "\n",
        "# Evaluate the model on the validation set\n",
        "val_preds = svm.predict(test_embeddings)\n",
        "val_acc = accuracy_score(test_labels, val_preds)\n",
        "val_f1 = f1_score(test_labels, val_preds, average='weighted')\n",
        "val_precision = precision_score(test_labels, val_preds, average='weighted')\n",
        "val_recall = recall_score(test_labels, val_preds, average='weighted')\n",
        "\n",
        "print(\"Validation set results:\")\n",
        "print(\"Accuracy: {:.4f}\".format(val_acc))\n",
        "print(\"F1 Score: {:.4f}\".format(val_f1))\n",
        "print(\"Precision: {:.4f}\".format(val_precision))\n",
        "print(\"Recall: {:.4f}\".format(val_recall))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "gather": {
          "logged": 1683407516751
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Load the data\n",
        "train_data = pd.read_csv(\"./lmr_train_mixed_labels.csv\")\n",
        "test_data = pd.read_csv(\"lmr_test.csv\")\n",
        "\n",
        "# Remove any rows with missing values\n",
        "train_data.dropna(inplace=True)\n",
        "test_data.dropna(inplace=True)\n",
        "\n",
        "train_dirt = train_data['review']\n",
        "test_dirt = test_data['review']\n",
        "\n",
        "train = list()\n",
        "test = list()\n",
        "\n",
        "train = clean_text(train_dirt, train)\n",
        "test = clean_text(test_dirt, test)\n",
        "\n",
        "train_data['tokens'] = train\n",
        "test_data['tokens'] = test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "gather": {
          "logged": 1683407517043
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "train_docs = [' '.join(sublist) for sublist in train]\n",
        "test_docs = [' '.join(sublist) for sublist in test]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1683406244012
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "print(\"Started Vectorization\")\n",
        "# vectorize text data\n",
        "vectorizer = CountVectorizer()\n",
        "train_bow = vectorizer.fit_transform(train)\n",
        "test_bow = vectorizer.transform(test)\n",
        "\n",
        "\n",
        "lda = LatentDirichletAllocation(n_components=50, random_state=42)\n",
        "lda.fit(train_bow)\n",
        "print(1)\n",
        "# Generate sparse vectors (word embeddings) for the train, validation and test sets\n",
        "train_embeddings = lda.transform(train_bow)\n",
        "print(2)\n",
        "test_embeddings = lda.transform(test_bow)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1683406244025
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "train_labels = train_data['label']\n",
        "test_labels = test_data['label']\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# Initialize an SVM classifier\n",
        "svm = SVC(random_state=42)\n",
        "\n",
        "# Fit the classifier on the training set\n",
        "svm.fit(train_embeddings, train_labels)\n",
        "\n",
        "# Evaluate the model on the validation set\n",
        "val_preds = svm.predict(test_embeddings)\n",
        "val_acc = accuracy_score(test_labels, val_preds)\n",
        "val_f1 = f1_score(test_labels, val_preds, average='weighted')\n",
        "val_precision = precision_score(test_labels, val_preds, average='weighted')\n",
        "val_recall = recall_score(test_labels, val_preds, average='weighted')\n",
        "\n",
        "print(\"Validation set results:\")\n",
        "print(\"Accuracy: {:.4f}\".format(val_acc))\n",
        "print(\"F1 Score: {:.4f}\".format(val_f1))\n",
        "print(\"Precision: {:.4f}\".format(val_precision))\n",
        "print(\"Recall: {:.4f}\".format(val_recall))\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "lr = LogisticRegression(random_state=42)\n",
        "lr.fit(train_embeddings, train_labels)\n",
        "\n",
        "# Evaluate the model on the validation set\n",
        "val_preds = lr.predict(test_embeddings)\n",
        "val_acc = accuracy_score(test_labels, val_preds)\n",
        "val_f1 = f1_score(test_labels, val_preds, average='weighted')\n",
        "val_precision = precision_score(test_labels, val_preds, average='weighted')\n",
        "val_recall = recall_score(test_labels, val_preds, average='weighted')\n",
        "\n",
        "print(\"Validation set results:\")\n",
        "print(\"Accuracy: {:.4f}\".format(val_acc))\n",
        "print(\"F1 Score: {:.4f}\".format(val_f1))\n",
        "print(\"Precision: {:.4f}\".format(val_precision))\n",
        "print(\"Recall: {:.4f}\".format(val_recall))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "gather": {
          "logged": 1683407525832
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "import gensim\n",
        "tagged_documents = []\n",
        "for i, doc in enumerate(train_docs):\n",
        "    tokens = gensim.utils.simple_preprocess(doc)\n",
        "    tagged_documents.append(TaggedDocument(tokens, [i]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "gather": {
          "logged": 1683407598711
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "model = Doc2Vec(tagged_documents, vector_size=400, window=5, min_count=1, workers=4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "gather": {
          "logged": 1683407772252
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "\n",
        "train_p_vectors = []\n",
        "for sentence in train_docs:\n",
        "    # Infer a new vector for the sentence\n",
        "    new_vector = model.infer_vector(sentence.split())\n",
        "    train_p_vectors.append(new_vector)\n",
        "\n",
        "\n",
        "test_p_vectors = []\n",
        "for sentence in test_docs:\n",
        "    # Infer a new vector for the sentence\n",
        "    new_vector = model.infer_vector(sentence.split())\n",
        "    test_p_vectors.append(new_vector)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "gather": {
          "logged": 1683407772451
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(50092, 400)\n",
            "(25060, 400)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "train_embeddings = np.array(train_p_vectors)\n",
        "print(train_embeddings.shape)\n",
        "test_embeddings = np.array(test_p_vectors)\n",
        "print(test_embeddings.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "train_labels = train_data['label'].apply(lambda x: 1 if x == 'pos' else 0)\n",
        "\n",
        "test_labels = test_data['label'].apply(lambda x: 1 if x == 'pos' else 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "gather": {
          "logged": 1683419663757
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation set results:\n",
            "Accuracy: 0.8074\n",
            "F1 Score: 0.8073\n",
            "Precision: 0.8079\n",
            "Recall: 0.8074\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/anaconda/envs/azureml_py38/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:938: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "lr = LogisticRegression(random_state=42)\n",
        "lr.fit(train_embeddings, train_labels)\n",
        "\n",
        "# Evaluate the model on the validation set\n",
        "val_preds = lr.predict(test_embeddings)\n",
        "val_acc = accuracy_score(test_labels, val_preds)\n",
        "val_f1 = f1_score(test_labels, val_preds, average='weighted')\n",
        "val_precision = precision_score(test_labels, val_preds, average='weighted')\n",
        "val_recall = recall_score(test_labels, val_preds, average='weighted')\n",
        "\n",
        "print(\"Validation set results:\")\n",
        "print(\"Accuracy: {:.4f}\".format(val_acc))\n",
        "print(\"F1 Score: {:.4f}\".format(val_f1))\n",
        "print(\"Precision: {:.4f}\".format(val_precision))\n",
        "print(\"Recall: {:.4f}\".format(val_recall))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "gather": {
          "logged": 1683421156916
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation set results:\n",
            "Accuracy: 0.8076\n",
            "F1 Score: 0.8076\n",
            "Precision: 0.8077\n",
            "Recall: 0.8076\n"
          ]
        }
      ],
      "source": [
        "train_labels = train_data['label']\n",
        "test_labels = test_data['label']\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# Initialize an SVM classifier\n",
        "svm = SVC(random_state=42)\n",
        "\n",
        "# Fit the classifier on the training set\n",
        "svm.fit(train_embeddings, train_labels)\n",
        "\n",
        "# Evaluate the model on the validation set\n",
        "val_preds = svm.predict(test_embeddings)\n",
        "val_acc = accuracy_score(test_labels, val_preds)\n",
        "val_f1 = f1_score(test_labels, val_preds, average='weighted')\n",
        "val_precision = precision_score(test_labels, val_preds, average='weighted')\n",
        "val_recall = recall_score(test_labels, val_preds, average='weighted')\n",
        "\n",
        "print(\"Validation set results:\")\n",
        "print(\"Accuracy: {:.4f}\".format(val_acc))\n",
        "print(\"F1 Score: {:.4f}\".format(val_f1))\n",
        "print(\"Precision: {:.4f}\".format(val_precision))\n",
        "print(\"Recall: {:.4f}\".format(val_recall))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1683406244124
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Load the data\n",
        "train_data = pd.read_csv(\"arp_train.csv\")\n",
        "test_data = pd.read_csv(\"arp_test.csv\")\n",
        "\n",
        "# Remove any rows with missing values\n",
        "train_data.dropna(inplace=True)\n",
        "test_data.dropna(inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1683406244136
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "clean_test = list()\n",
        "clean_text(test_data[\"review\"], clean_test)\n",
        "clean_train = list()\n",
        "clean_text(train_data[\"review\"], clean_train)\n",
        "\n",
        "train_docs = [' '.join(sublist) for sublist in clean_train]\n",
        "test_docs = [' '.join(sublist) for sublist in clean_test]\n",
        "print(\"done with cleaning data\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "train_labels = train_data['label'].apply(lambda x: 1 if x == 'pos' else 0)\n",
        "\n",
        "test_labels = test_data['label'].apply(lambda x: 1 if x == 'pos' else 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1683406244148
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "import gensim\n",
        "tagged_documents = []\n",
        "for i, doc in enumerate(train_docs):\n",
        "    tokens = gensim.utils.simple_preprocess(doc)\n",
        "    tagged_documents.append(TaggedDocument(tokens, [i]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1683406244161
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "model = Doc2Vec(tagged_documents, vector_size=100, window=5, min_count=1, workers=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1683406244181
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "train_p_vectors = []\n",
        "for sentence in train_docs:\n",
        "    # Infer a new vector for the sentence\n",
        "    new_vector = model.infer_vector(sentence.split())\n",
        "    train_p_vectors.append(new_vector)\n",
        "\n",
        "\n",
        "test_p_vectors = []\n",
        "for sentence in test_docs:\n",
        "    # Infer a new vector for the sentence\n",
        "    new_vector = model.infer_vector(sentence.split())\n",
        "    test_p_vectors.append(new_vector)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1683406244195
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "train_labels = train_data['label'].apply(lambda x: 1 if x == 'pos' else 0)\n",
        "\n",
        "test_labels = test_data['label'].apply(lambda x: 1 if x == 'pos' else 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1683406244207
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "train_embeddings = np.array(train_p_vectors)\n",
        "print(train_embeddings.shape)\n",
        "test_embeddings = np.array(test_p_vectors)\n",
        "print(test_embeddings.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1683406244219
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "lr = LogisticRegression(random_state=42)\n",
        "lr.fit(train_embeddings, train_labels)\n",
        "\n",
        "# Evaluate the model on the validation set\n",
        "val_preds = lr.predict(test_embeddings)\n",
        "val_acc = accuracy_score(test_labels, val_preds)\n",
        "val_f1 = f1_score(test_labels, val_preds, average='weighted')\n",
        "val_precision = precision_score(test_labels, val_preds, average='weighted')\n",
        "val_recall = recall_score(test_labels, val_preds, average='weighted')\n",
        "\n",
        "print(\"Validation set results:\")\n",
        "print(\"Accuracy: {:.4f}\".format(val_acc))\n",
        "print(\"F1 Score: {:.4f}\".format(val_f1))\n",
        "print(\"Precision: {:.4f}\".format(val_precision))\n",
        "print(\"Recall: {:.4f}\".format(val_recall))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "gather": {
          "logged": 1683419676027
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation set results:\n",
            "Accuracy: 0.8035\n",
            "F1 Score: 0.8030\n",
            "Precision: 0.8066\n",
            "Recall: 0.8035\n"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import SGDClassifier\n",
        "\n",
        "# Initialize an SVM classifier with SGD algorithm\n",
        "svm = SGDClassifier(loss='hinge', random_state=42)\n",
        "\n",
        "# Fit the classifier on the training set\n",
        "svm.fit(train_embeddings, train_labels)\n",
        "\n",
        "# Evaluate the model on the validation set\n",
        "val_preds = svm.predict(test_embeddings)\n",
        "val_acc = accuracy_score(test_labels, val_preds)\n",
        "val_f1 = f1_score(test_labels, val_preds, average='weighted')\n",
        "val_precision = precision_score(test_labels, val_preds, average='weighted')\n",
        "val_recall = recall_score(test_labels, val_preds, average='weighted')\n",
        "\n",
        "print(\"Validation set results:\")\n",
        "print(\"Accuracy: {:.4f}\".format(val_acc))\n",
        "print(\"F1 Score: {:.4f}\".format(val_f1))\n",
        "print(\"Precision: {:.4f}\".format(val_precision))\n",
        "print(\"Recall: {:.4f}\".format(val_recall))\n"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python38-azureml"
    },
    "kernelspec": {
      "display_name": "Python 3.8 - AzureML",
      "language": "python",
      "name": "python38-azureml"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      },
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
